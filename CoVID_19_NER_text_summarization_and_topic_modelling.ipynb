{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import json\n",
    "import glob\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from  collections import OrderedDict\n",
    "# Input data files are available in the \"../Coronavirus_19\" directory.\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta=pd.read_csv(\"/Users/patsnap/Desktop/Neo4J_and_other_codes/Coronavirus_19/CORD-19-research-challenge/metadata.csv\")\n",
    "meta.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta=meta[((meta['has_pdf_parse']==True) |(meta['has_pmc_xml_parse']==True))]\n",
    "meta_sm=meta[['cord_uid','sha','pmcid','title','abstract','publish_time','url']]\n",
    "meta_sm.drop_duplicates(subset =\"title\", keep = False, inplace = True)\n",
    "meta_sm.loc[meta_sm.publish_time=='2020-12-31'] = \"2020-03-31\"\n",
    "meta_sm.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "root_path = '/Users/patsnap/Desktop/Neo4J_and_other_codes/Coronavirus_19/CORD-19-research-challenge/'\n",
    "#inspired by this kernel. Thanks to the developer ref. https://www.kaggle.com/fmitchell259/create-corona-csv-file\n",
    "# Just set up a quick blank dataframe to hold all these medical papers. \n",
    "\n",
    "df = {\"paper_id\": [], \"text_body\": []}\n",
    "df = pd.DataFrame.from_dict(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_json = glob.glob(f'{root_path}/**/*.json', recursive=True)\n",
    "\n",
    "for i,file_name in enumerate (collect_json):\n",
    "    row = {\"paper_id\": None, \"text_body\": None}\n",
    "    if i%10000==0:\n",
    "        print (\"====processed \" + str(i)+ ' json files=====')\n",
    "        print()\n",
    "\n",
    "    with open(file_name) as json_data:\n",
    "            \n",
    "        data = json.load(json_data,object_pairs_hook=OrderedDict)\n",
    "        \n",
    "        row['paper_id']=data['paper_id']\n",
    "        \n",
    "        body_list = []\n",
    "       \n",
    "        for _ in range(len(data['body_text'])):\n",
    "            try:\n",
    "                body_list.append(data['body_text'][_]['text'])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        body = \"\\n \".join(body_list)\n",
    "        \n",
    "        row['text_body']=body \n",
    "        df = df.append(row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge metadata df with parsed json file based on sha_id\n",
    "merge1=pd.merge(meta_sm, df, left_on='sha', right_on=['paper_id'])\n",
    "merge1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merge1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge metadata set with parsed json file based on pcmid\n",
    "merge2=pd.merge(meta_sm, df, left_on='pmcid', right_on=['paper_id'])\n",
    "merge2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(merge2))\n",
    "#combine merged sha_id and pcmid dataset, remove the duplicate values based on file name\n",
    "merge_final= merge2.append(merge1, ignore_index=True)\n",
    "merge_final.drop_duplicates(subset =\"title\", keep = False, inplace = True)\n",
    "print(len(merge_final))\n",
    "merge_final.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove articles that are not related to COVID-19 based on publish time\n",
    "corona=merge_final[(merge_final['publish_time']>'2019-11-01') & (merge_final['text_body'].str.contains('nCoV|Cov|COVID|covid|SARS-CoV-2|sars-cov-2'))]\n",
    "corona.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "def clean_dataset(text):\n",
    "    text=re.sub('[\\[].*?[\\]]', '', str(text))  #remove in-text citation\n",
    "    text=re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '',text, flags=re.MULTILINE)#remove hyperlink\n",
    "    text=re.sub(r'\\\\b[A-Z a-z 0-9._ - ]*[@](.*?)[.]{1,3} \\\\b', '', text)#remove email\n",
    "    text=re.sub(r'^a1111111111 a1111111111 a1111111111 a1111111111 a1111111111.*[\\r\\n]*',' ',text)#have no idea what is a11111.. is, but I remove it now\n",
    "    text=re.sub(r'  +', ' ',text ) #remove extra space\n",
    "    text=re.sub('[,\\.!?]', '', text)\n",
    "    text=re.sub(r's/ ( *)/\\1/g','',text) \n",
    "    text=re.sub(r'[^\\w\\s]','',text) #strip punctions (recheck)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "corona['text_body'] =corona['text_body'].apply(clean_dataset)\n",
    "corona['title'] =corona['title'].apply(clean_dataset)\n",
    "corona['abstract'] =corona['abstract'].apply(clean_dataset)\n",
    "corona['text_body'] = corona['text_body'].map(lambda x: x.lower())\n",
    "coro=corona.reset_index(drop=True)\n",
    "coro.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coro['count_abstract'] = coro['abstract'].str.split().map(len)\n",
    "coro['count_abstract'].sort_values(ascending=True)\n",
    "#check word count\n",
    "y = np.array(coro['count_abstract'])\n",
    "sns.distplot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coro['count_text'] = coro['text_body'].str.split().map(len)\n",
    "coro['count_text'].sort_values(ascending=True)\n",
    "#check word count\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "y = np.array(coro['count_abstract'])\n",
    "sns.distplot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coro['count_text'] = coro['text_body'].str.split().map(len)\n",
    "coro['count_text'].sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coro['count_text'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(coro['count_text'])\n",
    "sns.distplot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coro2=coro[((coro['count_text']>500)&(coro['count_text']<4000))]\n",
    "coro2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coro2.to_csv(\"corona.csv\",index=False)\n",
    "#split articles w/o abstarct as the test dataset\n",
    "test=coro2[coro2['count_abstract']<5]\n",
    "test.head(2)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= coro2.drop(test.index)\n",
    "train.head(2)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.reset_index(drop=True)\n",
    "test=test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert extractive summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pypi.org/project/bert-extractive-summarizer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/dmmiller612/bert-extractive-summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bart summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gist.github.com/dizzySummer/0377bb6db284d3df45fdf75fe5394647#file-bart-summarization-ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "torch_device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "# load BART summarizer\n",
    "summarizer = pipeline(task=\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bart = torch.hub.load('pytorch/fairseq', 'bart.large')\n",
    "#bart.eval()  # disable dropout (or leave in train mode to finetune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('bart-large-cnn')\n",
    "model = BartForConditionalGeneration.from_pretrained('bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract=\"introduction an epidemic of coronavirus disease 2019 (covid-19) began in december 2019 in china leading to a public health emergency of international concern (pheic). clinical, laboratory, and imaging features have been partially characterized in some observational studies. no systematic reviews on covid-19 have been published to date. methods we performed a systematic literature review with meta-analysis, using three databases to assess clinical, laboratory, imaging features, and outcomes of covid-19 confirmed cases. observational studies and also case reports, were included, and analyzed separately. we performed a random-effects model meta-analysis to calculate the pooled prevalence and 95% confidence interval (95%ci). results 660 articles were retrieved for the time frame (1/1/2020-2/23/2020). after screening, 27 articles were selected for full-text assessment, 19 being finally included for qualitative and quantitative analyses. additionally, 39 case report articles were included and analyzed separately. for 656 patients, fever (88.7%, 95%ci 84.5–92.9%), cough (57.6%, 40.8–74.4%) and dyspnea (45.6%, 10.9–80.4%) were the most prevalent manifestations. among the patients, 20.3% (95%ci 10.0–30.6%) required intensive care unit (icu), 32.8% presented with acute respiratory distress syndrome (ards) (95%ci 13.7–51.8), 6.2% (95%ci 3.1–9.3) with shock. some 13.9% (95%ci 6.2–21.5%) of hospitalized patients had fatal outcomes (case fatality rate, cfr). conclusion covid-19 brings a huge burden to healthcare facilities, especially in patients with comorbidities. icu was required for approximately 20% of polymorbid, covid-19 infected patients and hospitalization was associated with a cfr of over 13%. as this virus spreads globally, countries need to urgently prepare human resources, infrastructure and facilities to treat severe covid-19.\"\n",
    "\n",
    "summary = summarizer(abstract, min_length=50, max_length=200)\n",
    "print (summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_summary = train.iloc[0:2,:]\n",
    "train_summary[\"text_summary\"] = train_summary[\"text_body\"].apply(lambda x: summarizer(x, min_length=50, max_length=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stop words\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "my_extra_stop_words = ['preprint','paper','copyright','case','also','moreover','use','from', 'subject', 're', 'edu', 'use','and','et','al','medrxiv','peerreviewed','peerreview','httpsdoiorg','license','authorfunder','grant','ccbyncnd','permission','grant','httpsdoiorg101101202002']\n",
    "\n",
    "train['text_body']=train['text_body'].apply(lambda x: ' '.join([word for word in x.split() if word not in (my_extra_stop_words) and word not in gensim.parsing.preprocessing.STOPWORDS and len(word)>3]))\n",
    "\n",
    "coronaRe=train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_sm\",disable=['parser', 'ner'])\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    text_out=[]\n",
    "    for word in texts:\n",
    "        data=nlp(word)\n",
    "        data=[word.lemma_ for word in data]\n",
    "        text_out.append(data)\n",
    "    return text_out\n",
    "coronaRe['new_lem'] = lemmatization(coronaRe['text_body'],allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "docs = coronaRe['new_lem']\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 10 documents, or more than 50% of the documents\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.5)\n",
    "\n",
    "# Create Bag-of-words representation of the documents\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))\n",
    "coronaRe.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "# Create Dictionary\n",
    "dictionary = gensim.corpora.Dictionary(coronaRe['new_lem'])\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    #print(k, v)\n",
    "    count += 1\n",
    "#less than 15 documents (absolute number) or more than 0.5 documents (fraction of total corpus size, not absolute number).after the above two steps, keep only the first 4500 most frequent tokens.\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=4500)\n",
    "# Create Corpus\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in coronaRe\n",
    "              ['new_lem']]\n",
    "bow_corpus_id=[ id for id in coronaRe['cord_uid']]\n",
    "# View\n",
    "#print(bow_corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=bow_corpus, id2word=dictionary, num_topics=10, random_state=100, chunksize=100, passes=10, per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]\n",
    "# Highest keyword probability is the topic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_df = lda_model.get_document_topics(bow_corpus,minimum_probability=0)\n",
    "lda_df = pd.DataFrame(list(lda_df))\n",
    "\n",
    "num_topics = lda_model.num_topics\n",
    "\n",
    "lda_df.columns = ['Topic'+str(i) for i in range(num_topics)]\n",
    "for i in range(len(lda_df.columns)):\n",
    "    lda_df.iloc[:,i]=lda_df.iloc[:,i].apply(lambda x: x[1])\n",
    "lda_df['Automated_topic_id'] =lda_df.apply(lambda x: np.argmax(x),axis=1)\n",
    "lda_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coherence score https://stackoverflow.com/questions/54762690/coherence-score-0-4-is-good-or-bad\n",
    "from gensim.models import CoherenceModel\n",
    "# Compute Coherence Score\n",
    "from tqdm import tqdm\n",
    "coherenceList_cv=[]\n",
    "num_topics_list = np.arange(5,26)\n",
    "for num_topics in tqdm(num_topics_list):\n",
    "    lda_model = gensim.models.LdaModel(corpus=bow_corpus, id2word=dictionary, num_topics=num_topics, random_state=100, chunksize=100, passes=10, alpha='auto', per_word_topics=True)\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=coronaRe['new_lem'], coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    coherenceList_cv.append(coherence_lda)\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-do (not correct)\n",
    "plotData = pd.DataFrame({'Number of topics':num_topics_list, 'CoherenceScore_cv':coherenceList_cv})\n",
    "f,ax = plt.subplots(figsize=(10,6))\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.pointplot(x='Number of topics', y= 'CoherenceScore_cv', data=plotData)\n",
    "plt.title('Topic coherence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final model\n",
    "\n",
    "Lda = gensim.models.LdaMulticore\n",
    "lda_final= Lda(corpus=bow_corpus, num_topics=17,id2word = dictionary, passes=10,chunksize=100,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# Print the Keyword in the 16 topics\n",
    "pprint(lda_final.print_topics())\n",
    "doc_lda = lda_final[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_df = lda_final.get_document_topics(bow_corpus,minimum_probability=0)\n",
    "lda_df = pd.DataFrame(list(lda_df))\n",
    "lda_id=pd.DataFrame(list(bow_corpus_id))\n",
    "num_topics = lda_final.num_topics\n",
    "\n",
    "lda_df.columns = ['Topic'+str(i) for i in range(num_topics)]\n",
    "\n",
    "for i in range(len(lda_df.columns)):\n",
    "    lda_df.iloc[:,i]=lda_df.iloc[:,i].apply(lambda x: x[1])\n",
    "\n",
    "lda_df['Automated_topic_id'] =lda_df.apply(lambda x: np.argmax(x),axis=1)\n",
    "\n",
    "lda_df['cord_uid']= lda_id\n",
    "lda_df[39:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic=lda_df[['Automated_topic_id','cord_uid']]\n",
    "plot_topics=lda_df.Automated_topic_id.value_counts().reset_index()\n",
    "plot_topics.columns=[\"topic_id\",\"quantity\"]\n",
    "plot_topics[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x=\"topic_id\", y=\"quantity\",  data=plot_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coronaRe['topic_id']= topic['Automated_topic_id']\n",
    "coronaRe.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER - Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_bionlp13cg_md-0.2.4.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "\n",
    "import en_ner_bionlp13cg_md\n",
    "nlp = en_ner_bionlp13cg_md.load()\n",
    "text = train['abstract'][2]\n",
    "doc = nlp(text)\n",
    "print(list(doc.sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(next(doc.sents), style='dep', jupyter=True,options = {'distance': 110})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

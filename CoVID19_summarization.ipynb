{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load library\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora, models\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51078, 18)\n"
     ]
    }
   ],
   "source": [
    "meta=pd.read_csv(\"/Users/patsnap/Desktop/Neo4J_and_other_codes/Coronavirus_19/CORD-19-research-challenge/metadata.csv\")\n",
    "print(meta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5330  papers are available after 2020 Jan 1.\n"
     ]
    }
   ],
   "source": [
    "### first filter by meta file. select only papers after 2020\n",
    "meta[\"publish_time\"] = pd.to_datetime(meta[\"publish_time\"])\n",
    "meta[\"publish_year\"] = (pd.DatetimeIndex(meta['publish_time']).year)\n",
    "meta[\"publish_month\"] = (pd.DatetimeIndex(meta['publish_time']).month)\n",
    "meta = meta[meta[\"publish_year\"] == 2020]\n",
    "print(meta.shape[0], \" papers are available after 2020 Jan 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3947  papers have abstract available.\n"
     ]
    }
   ],
   "source": [
    "#count how many has abstract\n",
    "count = 0\n",
    "index = []\n",
    "for i in range(len(meta)):\n",
    "    #print(i)\n",
    "    if type(meta.iloc[i, 8])== float:\n",
    "        count += 1\n",
    "    else:\n",
    "        index.append(i)\n",
    "\n",
    "print(len(index), \" papers have abstract available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Diabetes mellitus and hypertension are recogni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We detected bovine kobuvirus (BKV) in calves w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We examined nasal swabs and serum samples acqu...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  index\n",
       "0  Diabetes mellitus and hypertension are recogni...      0\n",
       "1  We detected bovine kobuvirus (BKV) in calves w...      1\n",
       "2  We examined nasal swabs and serum samples acqu...      2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##extract the abstract to pandas \n",
    "documents = meta.iloc[index, 8]\n",
    "documents=documents.reset_index()\n",
    "documents.drop(\"index\", inplace = True, axis = 1)\n",
    "\n",
    "##create pandas data frame with all abstracts, use as input corpus\n",
    "documents[\"index\"] = documents.index.values\n",
    "documents.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(400)\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##lemmatize and stemming\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            # TODO: Apply lemmatize_stemming on the token, then add to the results list\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document: \n",
      "['Abstract', 'Context', 'The', 'COVID-19', 'pandemic', 'created', 'a', 'rapid', 'and', 'unprecedented', 'shift', 'in', 'our', 'medical', 'system.', 'Medical', 'providers,', 'teams,', 'and', 'organizations', 'have', 'needed', 'to', 'shift', 'their', 'visits', 'away', 'from', 'face-to-face', 'visits', 'and', 'toward', 'telehealth', '(both', 'by', 'phone', 'and', 'through', 'video).', 'Palliative', 'care', 'teams', 'who', 'practice', 'in', 'the', 'community', 'setting', 'are', 'faced', 'with', 'a', 'difficult', 'task:', 'How', 'do', 'we', 'actively', 'triage', 'the', 'most', 'urgent', 'visits', 'while', 'keeping', 'our', 'vulnerable', 'patients', 'safe', 'from', 'the', 'pandemic?', 'Measures', 'The', 'following', 'are', 'recommendations', 'created', 'by', 'the', 'Palo', 'Alto', 'Medical', 'Foundation', 'Palliative', 'Care', 'and', 'Support', 'Services', 'team', 'to', 'help', 'triage', 'and', 'coordinate', 'for', 'timely,', 'safe,', 'and', 'effective', 'palliative', 'care', 'in', 'the', 'community', 'and', 'outpatient', 'setting', 'during', 'the', 'ongoing', 'COVID-19', 'pandemic.', 'Patients', 'are', 'initially', 'triaged', 'based', 'on', 'location', 'followed', 'by', 'acuity.', 'Interdisciplinary', 'care', 'is', 'implemented', 'using', 'strict', 'infection', 'control', 'guidelines', 'in', 'the', 'setting', 'of', 'limited', 'personal', 'protective', 'equipment', '(PPE)', 'resources.', 'We', 'implement', 'thorough', 'screening', 'for', 'COVID-19', 'symptoms', 'at', 'multiple', 'levels', 'before', 'a', 'patient', 'is', 'seen', 'by', 'a', 'designated', 'provider.', 'Conclusions/Lessons', 'Learned', 'We', 'recommend', 'active', 'triaging,', 'communication,', 'frequent', 'screening', 'for', 'COVID-19', 'symptoms', 'for', 'palliative', 'care', 'patients', 'been', 'evaluated', 'in', 'the', 'community', 'setting.', 'An', 'understanding', 'of', 'infection', 'risk,', 'mutual', 'consent', 'between', 'designated', 'providers,', 'patients,', 'and', 'their', 'families', 'are', 'crucial', 'to', 'maintaining', 'safety', 'while', 'delivering', 'community-based', 'palliative', 'care', 'during', 'the', 'COVID-19', 'pandemic.']\n",
      "\n",
      "\n",
      "Tokenized and lemmatized document: \n",
      "['abstract', 'context', 'covid', 'pandem', 'creat', 'rapid', 'unpreced', 'shift', 'medic', 'medic', 'provid', 'team', 'organ', 'need', 'shift', 'visit', 'away', 'face', 'face', 'visit', 'telehealth', 'phone', 'video', 'palliat', 'care', 'team', 'practic', 'communiti', 'set', 'face', 'difficult', 'task', 'activ', 'triag', 'urgent', 'visit', 'keep', 'vulner', 'patient', 'safe', 'pandem', 'measur', 'follow', 'recommend', 'creat', 'palo', 'alto', 'medic', 'foundat', 'palliat', 'care', 'support', 'servic', 'team', 'help', 'triag', 'coordin', 'time', 'safe', 'effect', 'palliat', 'care', 'communiti', 'outpati', 'set', 'ongo', 'covid', 'pandem', 'patient', 'initi', 'triag', 'base', 'locat', 'follow', 'acuiti', 'care', 'implement', 'strict', 'infect', 'control', 'guidelin', 'set', 'limit', 'person', 'protect', 'equip', 'resourc', 'implement', 'thorough', 'screen', 'covid', 'symptom', 'multipl', 'level', 'patient', 'see', 'design', 'provid', 'conclus', 'lesson', 'learn', 'recommend', 'activ', 'triag', 'communic', 'frequent', 'screen', 'covid', 'symptom', 'palliat', 'care', 'patient', 'evalu', 'communiti', 'set', 'understand', 'infect', 'risk', 'mutual', 'consent', 'design', 'provid', 'patient', 'famili', 'crucial', 'maintain', 'safeti', 'deliv', 'communiti', 'base', 'palliat', 'care', 'covid', 'pandem']\n"
     ]
    }
   ],
   "source": [
    "## use example to check the preprocessing step\n",
    "\n",
    "document_num = 1000  ##randomly pick one abstract\n",
    "doc_sample = documents[documents[\"index\"] == document_num].values[0][0]\n",
    "\n",
    "print(\"Original document: \")\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print(\"\\n\\nTokenized and lemmatized document: \")\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [diabet, mellitus, hypertens, recogn, risk, fa...\n",
       "1    [detect, bovin, kobuvirus, calv, diarrhea, uni...\n",
       "2    [examin, nasal, swab, serum, sampl, acquir, dr...\n",
       "3    [influenza, virus, potenti, caus, respiratori,...\n",
       "4    [cetuximab, improv, surviv, patient, metastat,...\n",
       "Name: abstract, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##preprocess all abstracts\n",
    "processed_docs = documents['abstract'].map(preprocess)\n",
    "processed_docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 arabia\n",
      "1 associ\n",
      "2 clinic\n",
      "3 close\n",
      "4 condit\n",
      "5 coronavirus\n"
     ]
    }
   ],
   "source": [
    "##create dictionary based on the preprocessed_documents\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "\n",
    "##check the dictionary\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove extreme words (very common and very rare)\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.1)\n",
    "\n",
    "##create bag-of-word model for each documents\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check the bow_corpus\n",
    "bow_doc_1000 = bow_corpus[document_num]\n",
    "\n",
    "# for i in range(len(bow_doc_1000)):\n",
    "#     print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_1000[i][0], dictionary[bow_doc_1000[i][0]], bow_doc_1000[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF_IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.35010075989000167),\n",
      " (1, 0.18684054739847109),\n",
      " (2, 0.19267453198919854),\n",
      " (3, 0.25956174363610257),\n",
      " (4, 0.2671209268243568),\n",
      " (5, 0.22469568206143628),\n",
      " (6, 0.2917165598339723),\n",
      " (7, 0.27834751937482777),\n",
      " (8, 0.1943534792734007),\n",
      " (9, 0.2077270875590725),\n",
      " (10, 0.18336543350016205),\n",
      " (11, 0.17845533155687388),\n",
      " (12, 0.24360370781665938),\n",
      " (13, 0.2720805833579731),\n",
      " (14, 0.34508200985963433),\n",
      " (15, 0.23401165060982926)]\n"
     ]
    }
   ],
   "source": [
    "#create tf-idf from bow_corpus\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "#preview the corpus_tfidf\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start model building at  2020-04-15 19:28:38\n",
      "Model training finished at  2020-04-15 19:30:48\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now()\n",
    "print (\"start model building at \",now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=5, id2word = dictionary, passes = 50, workers=4) \n",
    "now = datetime.datetime.now()\n",
    "print ('Model training finished at ',now.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.012*\"citi\" + 0.010*\"hubei\" + 0.009*\"travel\" + 0.009*\"march\" + 0.008*\"quarantin\" + 0.008*\"contact\" + 0.008*\"intervent\" + 0.008*\"individu\" + 0.007*\"reproduct\" + 0.007*\"social\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.014*\"vaccin\" + 0.012*\"sequenc\" + 0.011*\"genom\" + 0.010*\"drug\" + 0.010*\"host\" + 0.010*\"immun\" + 0.009*\"bind\" + 0.009*\"target\" + 0.009*\"express\" + 0.008*\"structur\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.010*\"research\" + 0.007*\"healthcar\" + 0.006*\"communiti\" + 0.005*\"recommend\" + 0.005*\"work\" + 0.005*\"practic\" + 0.005*\"articl\" + 0.005*\"chines\" + 0.005*\"nation\" + 0.005*\"support\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.011*\"sampl\" + 0.010*\"outcom\" + 0.008*\"children\" + 0.008*\"mortal\" + 0.008*\"critic\" + 0.008*\"fever\" + 0.008*\"blood\" + 0.007*\"negat\" + 0.007*\"swab\" + 0.007*\"admiss\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.020*\"imag\" + 0.018*\"chest\" + 0.017*\"lung\" + 0.014*\"lesion\" + 0.013*\"score\" + 0.012*\"featur\" + 0.010*\"fever\" + 0.010*\"grind\" + 0.010*\"famili\" + 0.009*\"anxieti\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##print out the key words of five topics\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the key words selected above, we can somehow summarized the five major topics as below:\n",
    "\n",
    "immunology\n",
    "hubei social, individual quarantin\n",
    "healthcare, recommendation\n",
    "genomic sequence\n",
    "symptoms (fever, chest image) + admision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF + LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "print (\"start model building at \",now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=5, id2word = dictionary, passes = 50, workers=4)\n",
    "now = datetime.datetime.now()\n",
    "print ('Model training finished at ',now.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check the key words of five topics\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print(\"Topic: {} Word: {}\".format(idx, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the keywords above, we can summarize the five topics as:\n",
    "\n",
    "healthcare and research,\n",
    "disease co-morbidities,\n",
    "Drug and genomic sequencing, biomedical\n",
    "Disease spread,\n",
    "Fever, chest image, symptoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply model to get all abstracts' topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_lda_topics = pd.DataFrame(columns = [\"topic1\", \"topic2\", \"topic3\", \"topic4\", \"topic5\"])\n",
    "documents_lda_tfidf_topics = pd.DataFrame(columns = [\"topic1\", \"topic2\", \"topic3\", \"topic4\", \"topic5\"])\n",
    "for i in range(len(bow_corpus)):\n",
    "    if i % 500 ==0:\n",
    "        print(i)\n",
    "    documents_lda_topics.loc[i] = [0] * 5\n",
    "    documents_lda_tfidf_topics.loc[i] = [0] * 5\n",
    "    \n",
    "    output = lda_model.get_document_topics(bow_corpus[i])\n",
    "    for j in range(len(output)):\n",
    "        a = output[j][0]\n",
    "        b = output[j][1]\n",
    "        documents_lda_topics.iloc[i,a] = b\n",
    "    \n",
    "    output_tfidf = lda_model_tfidf.get_document_topics(bow_corpus[i])\n",
    "    for k in range(len(output_tfidf)):\n",
    "        a = output_tfidf[k][0]\n",
    "        b = output_tfidf[k][1]\n",
    "        documents_lda_tfidf_topics.iloc[i, a] = b\n",
    "        \n",
    "print(\"Data processing finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pick the final topic for each abstract based on max-probability\n",
    "for i in range(5):\n",
    "    documents_lda_topics.iloc[:, i] = documents_lda_topics.iloc[:, i].astype('float64', copy=False)\n",
    "    \n",
    "documents_lda_topics[\"final_topic\"] =documents_lda_topics.iloc[:, :5].idxmax(axis=1)\n",
    "\n",
    "for i in range(5):\n",
    "    documents_lda_tfidf_topics.iloc[:, i] = documents_lda_tfidf_topics.iloc[:, i].astype('float64', copy=False)\n",
    "\n",
    "documents_lda_tfidf_topics[\"final_topic\"] =documents_lda_tfidf_topics.iloc[:, :5].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##preview the dataframe for both models\n",
    "print(\"LDA + bow_corpus: topic probability:\")\n",
    "documents_lda_topics.head(3)\n",
    "print(\"LDA + TF-IDF_corpus: topic probability:\")\n",
    "documents_lda_tfidf_topics.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca_result = pca.fit_transform(documents_lda_topics.iloc[:, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## with 3 components, variance explained\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create dataframe with projected vectors from PCA\n",
    "pca_df = pd.DataFrame()\n",
    "pca_df['pca-one'] = pca_result[:,0]\n",
    "pca_df['pca-two'] = pca_result[:,1] \n",
    "pca_df[\"pca-three\"] = pca_result[:, 2]\n",
    "pca_df[\"topic\"] = documents_lda_topics.iloc[:, 5].replace({\"topic1\": \"red\", \"topic2\": \"blue\", \"topic3\": \"green\", \"topic4\": \"yellow\", \"topic5\": \"black\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot( x=\"pca-one\", y=\"pca-two\", hue= documents_lda_topics.iloc[:, 5].replace({\"topic1\": \"red\", \"topic2\": \"blue\", \"topic3\": \"green\", \"topic4\": \"yellow\", \"topic5\": \"black\"}), data=pca_df, legend=\"full\", alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure(figsize=(16,10)).gca(projection='3d')\n",
    "ax.scatter(xs=pca_df[\"pca-one\"], ys=pca_df[\"pca-two\"], zs=pca_df[\"pca-three\"], cmap='tab10', c = documents_lda_topics.iloc[:, 5].replace({\"topic1\": \"red\", \"topic2\": \"blue\", \"topic3\": \"green\", \"topic4\": \"yellow\", \"topic5\": \"black\"}))\n",
    "ax.set_xlabel('pca-one')\n",
    "ax.set_ylabel('pca-two')\n",
    "ax.set_zlabel('pca-three')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##first run TSNE\n",
    "import time\n",
    "time_start = time.time()\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(documents_lda_topics.iloc[:, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create dataframe with TSNE results\n",
    "tsne_df = pd.DataFrame()\n",
    "tsne_df['tsne-2d-one'] = tsne_results[:,0]\n",
    "tsne_df['tsne-2d-two'] = tsne_results[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(x=\"tsne-2d-one\", y=\"tsne-2d-two\", hue=documents_lda_topics.iloc[:, 5].replace({\"topic1\": \"red\", \"topic2\": \"blue\", \"topic3\": \"green\", \"topic4\": \"yellow\", \"topic5\": \"black\"}), data=tsne_df, legend=\"full\", alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
